# Docker Environment Configuration
# Use container hostnames instead of localhost

# ============== Database Configuration ==============
POSTGRES_URI=postgresql+asyncpg://postgres:admin@postgres:5432/ats_db
POSTGRES_HOST=postgres
POSTGRES_PORT=5432
POSTGRES_USER=postgres
POSTGRES_PASSWORD=admin
POSTGRES_DB=ats_db

# ============== Neo4j Configuration ==============
NEO4J_URI=bolt://neo4j:7687
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=ats_neo4j_password

# ============== Ollama / LLM Configuration ==============
# Ollama runs on host machine - use host.docker.internal to access it
OLLAMA_BASE_URL=http://host.docker.internal:11434
LLM_MODEL=llama3.1:8b
LLM_EXTRACTION_MODEL=qwen2.5:3b
LLM_MAX_TOKENS=4096
LLM_TEMPERATURE=0.1
LLM_TIMEOUT=300

# ============== Provider Selection ==============
LLM_PROVIDER=ollama
GEMINI_API_KEY=
GEMINI_MODEL=gemini-flash-latest

# ============== Embedding Configuration ==============
EMBEDDING_MODEL=BAAI/bge-m3
EMBEDDING_DIM=1024
EMBEDDING_MAX_TOKENS=512

# ============== Reranking Configuration ==============
RERANK_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2

# ============== LightRAG Configuration ==============
RAG_WORKING_DIR=/app/rag_storage
CHUNK_TOKEN_SIZE=1200
CHUNK_OVERLAP_SIZE=200

# ============== API Configuration ==============
API_HOST=0.0.0.0
API_PORT=8000
API_DEBUG=false

# ============== Logging ==============
LOG_LEVEL=INFO
